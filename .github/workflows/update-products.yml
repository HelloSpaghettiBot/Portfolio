      - name: Fetch ALL products from Gumroad API (resilient, paginated, with image candidates)
        env:
          GUMROAD_ACCESS_TOKEN: ${{ secrets.GUMROAD_ACCESS_TOKEN }}
        run: |
          python - <<'PY'
          import os, json, time, sys, random
          from urllib import request, parse, error

          TOKEN = os.environ.get("GUMROAD_ACCESS_TOKEN")
          if not TOKEN:
              print("ERROR: Missing GUMROAD_ACCESS_TOKEN", file=sys.stderr)
              sys.exit(1)

          BASE = "https://api.gumroad.com/v2/products"
          UA   = "DigitalOutletSync/1.0 (+https://github.com/your-repo) GitHubActions"

          # -------- HTTP helpers with retry/backoff --------
          def http_get(url, max_attempts=8, base_delay=1.5, timeout=45):
              """
              GET with exponential backoff + jitter.
              Retries on 5xx, 504, 429, and network errors. Honors Retry-After if present.
              """
              last_err = None
              for attempt in range(1, max_attempts + 1):
                  req = request.Request(url, headers={
                      "User-Agent": UA,
                      "Accept": "application/json"
                  })
                  try:
                      with request.urlopen(req, timeout=timeout) as resp:
                          data = resp.read().decode("utf-8")
                          return json.loads(data)
                  except error.HTTPError as e:
                      # Handle rate limit or server errors
                      code = e.code
                      if code in (429, 500, 502, 503, 504):
                          retry_after = e.headers.get("Retry-After")
                          if retry_after:
                              try:
                                  delay = float(retry_after)
                              except:
                                  delay = base_delay * attempt
                          else:
                              # exponential backoff + jitter
                              delay = base_delay * (2 ** (attempt - 1)) + random.uniform(0, 0.75)
                          print(f"[WARN] HTTP {code} on attempt {attempt}/{max_attempts}; sleeping {delay:.1f}s")
                          time.sleep(delay)
                          last_err = e
                          continue
                      # Non-retryable HTTP error
                      print(f"[ERROR] HTTP {code} {e.reason} for {url}", file=sys.stderr)
                      raise
                  except error.URLError as e:
                      # Network hiccup; retry with backoff
                      delay = base_delay * (2 ** (attempt - 1)) + random.uniform(0, 0.75)
                      print(f"[WARN] Network error on attempt {attempt}/{max_attempts}: {e}; sleeping {delay:.1f}s")
                      time.sleep(delay)
                      last_err = e
                      continue
                  except Exception as e:
                      # Unknown; retry a few times anyway
                      delay = base_delay * (2 ** (attempt - 1)) + random.uniform(0, 0.75)
                      print(f"[WARN] Unexpected error on attempt {attempt}/{max_attempts}: {e}; sleeping {delay:.1f}s")
                      time.sleep(delay)
                      last_err = e
                      continue
              # exhausted
              print("[ERROR] Exhausted retries", file=sys.stderr)
              if last_err:
                  raise last_err
              raise RuntimeError("Unknown GET failure")

          # -------- Pull all pages safely --------
          all_items = []
          seen_ids  = set()
          page      = 1
          PER_PAGE  = 100
          HARD_CAP  = 200  # safety valve to avoid infinite loops

          while page <= HARD_CAP:
              params = {"access_token": TOKEN, "page": page, "per_page": PER_PAGE}
              url = BASE + "?" + parse.urlencode(params)
              data = http_get(url)

              if not data.get("success"):
                  print(f"[WARN] API returned success=false on page {page}")
                  break

              chunk = data.get("products") or []
              if not chunk:
                  print(f"Page {page}: empty -> stop")
                  break

              # keep only new IDs — prevents looping when 'page' is ignored
              new = [p for p in chunk if p.get("id") not in seen_ids]
              for p in new:
                  seen_ids.add(p["id"])
                  all_items.append(p)

              print(f"Page {page}: {len(chunk)} items, {len(new)} new, total unique {len(all_items)}")

              if not new:
                  print("No new items on this page -> stop (pagination likely ignored).")
                  break

              if data.get("has_more") is False:
                  print("has_more = false -> stop")
                  break

              page += 1
              # light pacing to be nice to API
              time.sleep(0.2)

          # -------- Normalize -> include multiple image candidates --------
          out = []
          for p in all_items:
              title = p.get("name") or "Untitled"
              url   = p.get("short_url") or p.get("url") or ""
              imgs  = []
              for key in ("preview_url", "cover_image_url", "thumbnail_url"):
                  v = p.get(key)
                  if v and v not in imgs:
                      imgs.append(v)
              if not imgs:
                  # stable placeholder seeded by title
                  seed = parse.quote(title)
                  imgs.append(f"https://picsum.photos/seed/{seed}/800/800")

              out.append({"title": title, "url": url, "images": imgs})

          # Deduplicate conservatively by (url, title)
          seen = set()
          dedup = []
          for item in out:
              key = (item["url"], item["title"])
              if key in seen: 
                  continue
              seen.add(key)
              dedup.append(item)

          with open("products.json", "w", encoding="utf-8") as f:
              json.dump(dedup, f, indent=2, ensure_ascii=False)

          print(f"✅ Saved {len(dedup)} products to products.json")
          PY
